{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert - Topic.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMw5XRPieeOLlut1+Fd77p5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mxn170019/ML-Excercises/blob/master/Bert_Topic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqEuvfbczbuC"
      },
      "source": [
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "data = fetch_20newsgroups(subset='train')['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToC_mcwAzfy_"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXUxwmk6z45e"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-7H7Y1c0MuE"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "embeddings = model.encode(data, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd904Wfq1AY2"
      },
      "source": [
        "embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H82xz0ai0Szq"
      },
      "source": [
        "#!pip install umap-learn\n",
        "#!pip install hdbscan "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX_FpDaU0ris"
      },
      "source": [
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5, \n",
        "                            metric='cosine').fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cJbssib1SMi"
      },
      "source": [
        "import hdbscan\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
        "                          metric='euclidean',                      \n",
        "                          cluster_selection_method='eom').fit(umap_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AGld1tG1Yz-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data\n",
        "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
        "result['labels'] = cluster.labels_\n",
        "\n",
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XASJtz9X3F-Q"
      },
      "source": [
        "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
        "docs_df['Topic'] = cluster.labels_\n",
        "docs_df['Doc_ID'] = range(len(docs_df))\n",
        "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbi5DOjR3Iqe"
      },
      "source": [
        "docs_per_topic.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXPxAxbE3Rgi"
      },
      "source": [
        "docs_per_topic.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPpdDnq31meu"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "  \n",
        "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zewy3YB131KQ"
      },
      "source": [
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRh8yPkl4o-v"
      },
      "source": [
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['Topic'])\n",
        "                     .Doc\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPhR2TD4r3t"
      },
      "source": [
        "top_n_words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_2wx-n25Vw-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQU0qbQ4v3P"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "for i in range(20):\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(tf_idf.T)\n",
        "    np.fill_diagonal(similarities, 0)\n",
        "\n",
        "    # Extract label to merge into and from where\n",
        "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
        "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
        "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
        "\n",
        "    # Adjust topics\n",
        "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
        "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
        "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
        "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
        "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "\n",
        "    # Calculate new topic words\n",
        "    m = len(data)\n",
        "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
        "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "\n",
        "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbp-NP6G5Uly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}